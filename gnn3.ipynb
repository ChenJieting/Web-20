{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Graph Neural Networks - III\n",
        "====================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Heterogeneous Graphs\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Each value of the dictionary is a list of edge tuples.\n",
        "# Nodes are integer IDs starting from zero. Nodes IDs of different types have\n",
        "# separate countings.\n",
        "import dgl\n",
        "\n",
        "ratings = dgl.heterograph(\n",
        "    {('user', '+1', 'movie') : [(0, 0), (0, 1), (1, 0)],\n",
        "     ('user', '-1', 'movie') : [(2, 1)]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Creating from scipy matrix\n",
        "import scipy.sparse as sp\n",
        "\n",
        "plus1 = sp.coo_matrix(([1, 1, 1], ([0, 0, 1], [0, 1, 0])), shape=(3, 2))\n",
        "minus1 = sp.coo_matrix(([1], ([2], [1])), shape=(3, 2))\n",
        "ratings = dgl.heterograph(\n",
        "    {('user', '+1', 'movie') : plus1,\n",
        "     ('user', '-1', 'movie') : minus1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['__header__', '__version__', '__globals__', 'TvsP', 'PvsA', 'PvsV', 'AvsF', 'VvsC', 'PvsL', 'PvsC', 'A', 'C', 'F', 'L', 'P', 'T', 'V', 'PvsT', 'CNormPvsA', 'RNormPvsA', 'CNormPvsC', 'RNormPvsC', 'CNormPvsT', 'RNormPvsT', 'CNormPvsV', 'RNormPvsV', 'CNormVvsC', 'RNormVvsC', 'CNormAvsF', 'RNormAvsF', 'CNormPvsL', 'RNormPvsL', 'stopwords', 'nPvsT', 'nT', 'CNormnPvsT', 'RNormnPvsT', 'nnPvsT', 'nnT', 'CNormnnPvsT', 'RNormnnPvsT', 'PvsP', 'CNormPvsP', 'RNormPvsP']\n"
        }
      ],
      "source": [
        "import scipy.io\n",
        "import urllib.request\n",
        "\n",
        "data_url = 'https://data.dgl.ai/dataset/ACM.mat'\n",
        "data_file_path = './ACM.mat'\n",
        "\n",
        "#urllib.request.urlretrieve(data_url, data_file_path)\n",
        "data = scipy.io.loadmat(data_file_path)\n",
        "print(list(data.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'scipy.sparse.csc.csc_matrix'>\n#Papers: 12499\n#Authors: 14\n#Links: 12499\n"
        }
      ],
      "source": [
        "print(type(data['PvsA']))\n",
        "print('#Papers:', data['PvsC'].shape[0])\n",
        "print('#Authors:', data['PvsC'].shape[1])\n",
        "print('#Links:', data['PvsC'].nnz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "pvc = data['PvsC'].tocsr()\n",
        "# find all papers published in KDD, ICML, VLDB\n",
        "c_selected = [0, 11, 13]  # KDD, ICML, VLDB\n",
        "p_selected = pvc[:, c_selected].tocoo()\n",
        "# generate labels\n",
        "labels = pvc.indices\n",
        "labels[labels == 11] = 1\n",
        "labels[labels == 13] = 2\n",
        "labels = torch.tensor(labels).long()\n",
        "\n",
        "# generate train/val/test split\n",
        "pid = p_selected.row\n",
        "shuffle = np.random.permutation(pid)\n",
        "train_idx = torch.tensor(shuffle[0:800]).long()\n",
        "val_idx = torch.tensor(shuffle[800:900]).long()\n",
        "test_idx = torch.tensor(shuffle[900:]).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(2094,)\n(12499, 3)\n(12499, 14)\n2094\n"
        }
      ],
      "source": [
        "print(pid.shape)\n",
        "print(p_selected.shape)\n",
        "print(pvc.shape)\n",
        "print(p_selected.nnz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dgl.function as fn\n",
        "\n",
        "class HeteroRGCNLayer(nn.Module):\n",
        "    def __init__(self, in_size, out_size, etypes):\n",
        "        super(HeteroRGCNLayer, self).__init__()\n",
        "        # W_r for each relation\n",
        "        self.weight = nn.ModuleDict({\n",
        "                name : nn.Linear(in_size, out_size) for name in etypes\n",
        "            })\n",
        "\n",
        "    def forward(self, G, feat_dict):\n",
        "        # The input is a dictionary of node features for each type\n",
        "        funcs = {}\n",
        "        for srctype, etype, dsttype in G.canonical_etypes:\n",
        "            # Compute W_r * h\n",
        "            Wh = self.weight[etype](feat_dict[srctype])\n",
        "            # Save it in graph for message passing\n",
        "            G.nodes[srctype].data['Wh_%s' % etype] = Wh\n",
        "            # Specify per-relation message passing functions: (message_func, reduce_func).\n",
        "            # Note that the results are saved to the same destination feature 'h', which\n",
        "            # hints the type wise reducer for aggregation.\n",
        "            funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n",
        "        # Trigger message passing of multiple types.\n",
        "        # The first argument is the message passing functions for each relation.\n",
        "        # The second one is the type wise reducer, could be \"sum\", \"max\",\n",
        "        # \"min\", \"mean\", \"stack\"\n",
        "        G.multi_update_all(funcs, 'sum')\n",
        "        # return the updated node feature dictionary\n",
        "        return {ntype : G.nodes[ntype].data['h'] for ntype in G.ntypes}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HeteroRGCN(nn.Module):\n",
        "    def __init__(self, G, in_size, hidden_size, out_size):\n",
        "        super(HeteroRGCN, self).__init__()\n",
        "        # Use trainable node embeddings as featureless inputs.\n",
        "        embed_dict = {ntype : nn.Parameter(torch.Tensor(G.number_of_nodes(ntype), in_size))\n",
        "                      for ntype in G.ntypes}\n",
        "        for key, embed in embed_dict.items():\n",
        "            nn.init.xavier_uniform_(embed)\n",
        "        self.embed = nn.ParameterDict(embed_dict)\n",
        "        # create layers\n",
        "        self.layer1 = HeteroRGCNLayer(in_size, hidden_size, G.etypes)\n",
        "        self.layer2 = HeteroRGCNLayer(hidden_size, out_size, G.etypes)\n",
        "\n",
        "    def forward(self, G):\n",
        "        h_dict = self.layer1(G, self.embed)\n",
        "        h_dict = {k : F.leaky_relu(h) for k, h in h_dict.items()}\n",
        "        h_dict = self.layer2(G, h_dict)\n",
        "        # get paper logits\n",
        "        return h_dict['paper']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Graph(num_nodes={'paper': 12499, 'author': 17431, 'subject': 73},\n      num_edges={('paper', 'written-by', 'author'): 37055, ('author', 'writing', 'paper'): 37055, ('paper', 'citing', 'paper'): 30789, ('paper', 'cited', 'paper'): 30789, ('paper', 'is-about', 'subject'): 12499, ('subject', 'has', 'paper'): 12499},\n      metagraph=[('paper', 'author'), ('paper', 'paper'), ('paper', 'paper'), ('paper', 'subject'), ('author', 'paper'), ('subject', 'paper')])\n"
        }
      ],
      "source": [
        "G = dgl.heterograph({\n",
        "        ('paper', 'written-by', 'author') : data['PvsA'],\n",
        "        ('author', 'writing', 'paper') : data['PvsA'].transpose(),\n",
        "        ('paper', 'citing', 'paper') : data['PvsP'],\n",
        "        ('paper', 'cited', 'paper') : data['PvsP'].transpose(),\n",
        "        ('paper', 'is-about', 'subject') : data['PvsL'],\n",
        "        ('subject', 'has', 'paper') : data['PvsL'].transpose(),\n",
        "    })\n",
        "\n",
        "print(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loss 1.0530, Train Acc 0.4938, Val Acc 0.5300 (Best 0.5300), Test Acc 0.5008 (Best 0.5008)\nLoss 0.9602, Train Acc 0.4975, Val Acc 0.5500 (Best 0.5500), Test Acc 0.5092 (Best 0.5092)\nLoss 0.8623, Train Acc 0.5100, Val Acc 0.5500 (Best 0.5500), Test Acc 0.5092 (Best 0.5092)\nLoss 0.7001, Train Acc 0.8025, Val Acc 0.6700 (Best 0.6700), Test Acc 0.6466 (Best 0.6466)\nLoss 0.4846, Train Acc 0.9200, Val Acc 0.7500 (Best 0.7500), Test Acc 0.7362 (Best 0.7127)\nLoss 0.2927, Train Acc 0.9663, Val Acc 0.8000 (Best 0.8000), Test Acc 0.7739 (Best 0.7580)\nLoss 0.1707, Train Acc 0.9862, Val Acc 0.7800 (Best 0.8000), Test Acc 0.7764 (Best 0.7580)\nLoss 0.1056, Train Acc 0.9912, Val Acc 0.7700 (Best 0.8000), Test Acc 0.7714 (Best 0.7580)\nLoss 0.0742, Train Acc 0.9962, Val Acc 0.7700 (Best 0.8000), Test Acc 0.7722 (Best 0.7580)\nLoss 0.0577, Train Acc 1.0000, Val Acc 0.7600 (Best 0.8000), Test Acc 0.7688 (Best 0.7580)\nLoss 0.0470, Train Acc 1.0000, Val Acc 0.7400 (Best 0.8000), Test Acc 0.7680 (Best 0.7580)\nLoss 0.0396, Train Acc 1.0000, Val Acc 0.7600 (Best 0.8000), Test Acc 0.7672 (Best 0.7580)\nLoss 0.0341, Train Acc 1.0000, Val Acc 0.7600 (Best 0.8000), Test Acc 0.7688 (Best 0.7580)\nLoss 0.0295, Train Acc 1.0000, Val Acc 0.7700 (Best 0.8000), Test Acc 0.7730 (Best 0.7580)\nLoss 0.0258, Train Acc 1.0000, Val Acc 0.7600 (Best 0.8000), Test Acc 0.7705 (Best 0.7580)\nLoss 0.0230, Train Acc 1.0000, Val Acc 0.7700 (Best 0.8000), Test Acc 0.7714 (Best 0.7580)\nLoss 0.0210, Train Acc 1.0000, Val Acc 0.7700 (Best 0.8000), Test Acc 0.7739 (Best 0.7580)\nLoss 0.0196, Train Acc 1.0000, Val Acc 0.7700 (Best 0.8000), Test Acc 0.7772 (Best 0.7580)\nLoss 0.0184, Train Acc 1.0000, Val Acc 0.7800 (Best 0.8000), Test Acc 0.7772 (Best 0.7580)\nLoss 0.0173, Train Acc 1.0000, Val Acc 0.7700 (Best 0.8000), Test Acc 0.7755 (Best 0.7580)\n"
        }
      ],
      "source": [
        "# Create the model. The output has three logits for three classes.\n",
        "model = HeteroRGCN(G, 10, 10, 3)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "best_val_acc = 0\n",
        "best_test_acc = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "    logits = model(G)\n",
        "    # The loss is computed only for labeled nodes.\n",
        "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
        "\n",
        "    pred = logits.argmax(1)\n",
        "    train_acc = (pred[train_idx] == labels[train_idx]).float().mean()\n",
        "    val_acc = (pred[val_idx] == labels[val_idx]).float().mean()\n",
        "    test_acc = (pred[test_idx] == labels[test_idx]).float().mean()\n",
        "\n",
        "    if best_val_acc < val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_test_acc = test_acc\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print('Loss %.4f, Train Acc %.4f, Val Acc %.4f (Best %.4f), Test Acc %.4f (Best %.4f)' % (\n",
        "            loss.item(),\n",
        "            train_acc.item(),\n",
        "            val_acc.item(),\n",
        "            best_val_acc.item(),\n",
        "            test_acc.item(),\n",
        "            best_test_acc.item(),\n",
        "        ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RGCN\n",
        "------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl import DGLGraph\n",
        "import dgl.function as fn\n",
        "from functools import partial\n",
        "\n",
        "class RGCNLayer(nn.Module):\n",
        "    def __init__(self, in_feat, out_feat, num_rels, num_bases=-1, bias=None,\n",
        "                 activation=None, is_input_layer=False):\n",
        "        super(RGCNLayer, self).__init__()\n",
        "        self.in_feat = in_feat\n",
        "        self.out_feat = out_feat\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "        self.is_input_layer = is_input_layer\n",
        "\n",
        "        # sanity check\n",
        "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
        "            self.num_bases = self.num_rels\n",
        "\n",
        "        # weight bases in equation (3)\n",
        "        self.weight = nn.Parameter(torch.Tensor(self.num_bases, self.in_feat,\n",
        "                                                self.out_feat))\n",
        "        if self.num_bases < self.num_rels:\n",
        "            # linear combination coefficients in equation (3)\n",
        "            self.w_comp = nn.Parameter(torch.Tensor(self.num_rels, self.num_bases))\n",
        "\n",
        "        # add bias\n",
        "        if self.bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
        "\n",
        "        # init trainable parameters\n",
        "        nn.init.xavier_uniform_(self.weight,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "        if self.num_bases < self.num_rels:\n",
        "            nn.init.xavier_uniform_(self.w_comp,\n",
        "                                    gain=nn.init.calculate_gain('relu'))\n",
        "        if self.bias:\n",
        "            nn.init.xavier_uniform_(self.bias,\n",
        "                                    gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def forward(self, g):\n",
        "        if self.num_bases < self.num_rels:\n",
        "            # generate all weights from bases (equation (3))\n",
        "            weight = self.weight.view(self.in_feat, self.num_bases, self.out_feat)\n",
        "            weight = torch.matmul(self.w_comp, weight).view(self.num_rels,\n",
        "                                                        self.in_feat, self.out_feat)\n",
        "        else:\n",
        "            weight = self.weight\n",
        "\n",
        "        if self.is_input_layer:\n",
        "            def message_func(edges):\n",
        "                # for input layer, matrix multiply can be converted to be\n",
        "                # an embedding lookup using source node id\n",
        "                embed = weight.view(-1, self.out_feat)\n",
        "                index = edges.data['rel_type'] * self.in_feat + edges.src['id']\n",
        "                return {'msg': embed[index] * edges.data['norm']}\n",
        "        else:\n",
        "            def message_func(edges):\n",
        "                i = edges.data['rel_type'].type(torch.long)\n",
        "                w = weight[i]\n",
        "                msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
        "                msg = msg * edges.data['norm']\n",
        "                return {'msg': msg}\n",
        "\n",
        "        def apply_func(nodes):\n",
        "            h = nodes.data['h']\n",
        "            if self.bias:\n",
        "                h = h + self.bias\n",
        "            if self.activation:\n",
        "                h = self.activation(h)\n",
        "            return {'h': h}\n",
        "\n",
        "        g.update_all(message_func, fn.sum(msg='msg', out='h'), apply_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Full R-GCN model defined\n",
        "~~~~~~~~~~~~~~~~~~~~~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,\n",
        "                 num_bases=-1, num_hidden_layers=1):\n",
        "        super(Model, self).__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.h_dim = h_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "\n",
        "        # create rgcn layers\n",
        "        self.build_model()\n",
        "\n",
        "        # create initial features\n",
        "        self.features = self.create_features()\n",
        "\n",
        "    def build_model(self):\n",
        "        self.layers = nn.ModuleList()\n",
        "        # input to hidden\n",
        "        i2h = self.build_input_layer()\n",
        "        self.layers.append(i2h)\n",
        "        # hidden to hidden\n",
        "        for _ in range(self.num_hidden_layers):\n",
        "            h2h = self.build_hidden_layer()\n",
        "            self.layers.append(h2h)\n",
        "        # hidden to output\n",
        "        h2o = self.build_output_layer()\n",
        "        self.layers.append(h2o)\n",
        "\n",
        "    # initialize feature for each node\n",
        "    def create_features(self):\n",
        "        features = torch.arange(self.num_nodes)\n",
        "        return features\n",
        "\n",
        "    def build_input_layer(self):\n",
        "        return RGCNLayer(self.num_nodes, self.h_dim, self.num_rels, self.num_bases,\n",
        "                         activation=F.relu, is_input_layer=True)\n",
        "\n",
        "    def build_hidden_layer(self):\n",
        "        return RGCNLayer(self.h_dim, self.h_dim, self.num_rels, self.num_bases,\n",
        "                         activation=F.relu)\n",
        "\n",
        "    def build_output_layer(self):\n",
        "        return RGCNLayer(self.h_dim, self.out_dim, self.num_rels, self.num_bases,\n",
        "                         activation=partial(F.softmax, dim=1))\n",
        "\n",
        "    def forward(self, g):\n",
        "        if self.features is not None:\n",
        "            g.ndata['id'] = self.features\n",
        "        for layer in self.layers:\n",
        "            layer(g)\n",
        "        return g.ndata.pop('h')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Handle dataset\n",
        "~~~~~~~~~~~~~~~~\n",
        "This tutorial uses Institute for Applied Informatics and Formal Description Methods (AIFB) dataset from R-GCN paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading dataset aifb\nNumber of nodes:  8285\nNumber of edges:  66371\nNumber of relations:  91\nNumber of classes:  4\nremoving nodes that are more than 3 hops away\n"
        }
      ],
      "source": [
        "# load graph data\n",
        "from dgl.contrib.data import load_data\n",
        "import numpy as np\n",
        "import torch\n",
        "data = load_data(dataset='aifb')\n",
        "num_nodes = data.num_nodes\n",
        "num_rels = data.num_rels\n",
        "num_classes = data.num_classes\n",
        "labels = data.labels\n",
        "train_idx = data.train_idx\n",
        "# split training and validation set\n",
        "val_idx = train_idx[:len(train_idx) // 5]\n",
        "train_idx = train_idx[len(train_idx) // 5:]\n",
        "\n",
        "# edge type and normalization factor\n",
        "edge_type = torch.from_numpy(data.edge_type)\n",
        "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
        "\n",
        "labels = torch.from_numpy(labels).view(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create graph and model\n",
        "~~~~~~~~~~~~~~~~~~~~~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configurations\n",
        "n_hidden = 16 # number of hidden units\n",
        "n_bases = -1 # use number of relations as number of bases\n",
        "n_hidden_layers = 0 # use 1 input layer, 1 output layer, no hidden layer\n",
        "n_epochs = 25 # epochs to train\n",
        "lr = 0.01 # learning rate\n",
        "l2norm = 0 # L2 norm coefficient\n",
        "\n",
        "# create graph\n",
        "g = DGLGraph((data.edge_src, data.edge_dst))\n",
        "g.edata.update({'rel_type': edge_type, 'norm': edge_norm})\n",
        "\n",
        "# create model\n",
        "model = Model(len(g),\n",
        "              n_hidden,\n",
        "              num_classes,\n",
        "              num_rels,\n",
        "              num_bases=n_bases,\n",
        "              num_hidden_layers=n_hidden_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training loop\n",
        "~~~~~~~~~~~~~~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "start training...\nEpoch 00000 | Train Accuracy: 0.3214 | Train Loss: 1.3857 | Validation Accuracy: 0.3214 | Validation loss: 1.3858\nEpoch 00001 | Train Accuracy: 0.9375 | Train Loss: 1.3354 | Validation Accuracy: 0.8929 | Validation loss: 1.3498\nEpoch 00002 | Train Accuracy: 0.9375 | Train Loss: 1.2598 | Validation Accuracy: 0.9643 | Validation loss: 1.2983\nEpoch 00003 | Train Accuracy: 0.9464 | Train Loss: 1.1660 | Validation Accuracy: 0.9643 | Validation loss: 1.2310\nEpoch 00004 | Train Accuracy: 0.9464 | Train Loss: 1.0742 | Validation Accuracy: 0.9643 | Validation loss: 1.1530\nEpoch 00005 | Train Accuracy: 0.9464 | Train Loss: 0.9993 | Validation Accuracy: 0.9643 | Validation loss: 1.0735\nEpoch 00006 | Train Accuracy: 0.9554 | Train Loss: 0.9413 | Validation Accuracy: 0.9643 | Validation loss: 1.0009\nEpoch 00007 | Train Accuracy: 0.9554 | Train Loss: 0.8965 | Validation Accuracy: 0.9643 | Validation loss: 0.9409\nEpoch 00008 | Train Accuracy: 0.9554 | Train Loss: 0.8618 | Validation Accuracy: 0.9643 | Validation loss: 0.8943\nEpoch 00009 | Train Accuracy: 0.9554 | Train Loss: 0.8355 | Validation Accuracy: 0.9643 | Validation loss: 0.8597\nEpoch 00010 | Train Accuracy: 0.9554 | Train Loss: 0.8164 | Validation Accuracy: 0.9643 | Validation loss: 0.8345\nEpoch 00011 | Train Accuracy: 0.9643 | Train Loss: 0.8029 | Validation Accuracy: 0.9643 | Validation loss: 0.8169\nEpoch 00012 | Train Accuracy: 0.9643 | Train Loss: 0.7932 | Validation Accuracy: 0.9643 | Validation loss: 0.8050\nEpoch 00013 | Train Accuracy: 0.9821 | Train Loss: 0.7856 | Validation Accuracy: 0.9643 | Validation loss: 0.7972\nEpoch 00014 | Train Accuracy: 0.9821 | Train Loss: 0.7792 | Validation Accuracy: 0.9643 | Validation loss: 0.7922\nEpoch 00015 | Train Accuracy: 0.9821 | Train Loss: 0.7739 | Validation Accuracy: 0.9643 | Validation loss: 0.7892\nEpoch 00016 | Train Accuracy: 0.9821 | Train Loss: 0.7697 | Validation Accuracy: 0.9643 | Validation loss: 0.7875\nEpoch 00017 | Train Accuracy: 0.9821 | Train Loss: 0.7665 | Validation Accuracy: 0.9643 | Validation loss: 0.7868\nEpoch 00018 | Train Accuracy: 0.9821 | Train Loss: 0.7641 | Validation Accuracy: 0.9643 | Validation loss: 0.7867\nEpoch 00019 | Train Accuracy: 0.9821 | Train Loss: 0.7622 | Validation Accuracy: 0.9643 | Validation loss: 0.7872\nEpoch 00020 | Train Accuracy: 0.9821 | Train Loss: 0.7605 | Validation Accuracy: 0.9643 | Validation loss: 0.7882\nEpoch 00021 | Train Accuracy: 0.9821 | Train Loss: 0.7589 | Validation Accuracy: 0.9643 | Validation loss: 0.7896\nEpoch 00022 | Train Accuracy: 0.9821 | Train Loss: 0.7571 | Validation Accuracy: 0.9643 | Validation loss: 0.7916\nEpoch 00023 | Train Accuracy: 0.9821 | Train Loss: 0.7552 | Validation Accuracy: 0.9643 | Validation loss: 0.7941\nEpoch 00024 | Train Accuracy: 0.9911 | Train Loss: 0.7531 | Validation Accuracy: 0.9643 | Validation loss: 0.7974\n"
        }
      ],
      "source": [
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
        "\n",
        "print(\"start training...\")\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model.forward(g)\n",
        "    loss = F.cross_entropy(logits[train_idx], labels[train_idx].long())\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
        "    train_acc = train_acc.item() / len(train_idx)\n",
        "    val_loss = F.cross_entropy(logits[val_idx], labels[val_idx].long())\n",
        "    val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx])\n",
        "    val_acc = val_acc.item() / len(val_idx)\n",
        "    print(\"Epoch {:05d} | \".format(epoch) +\n",
        "          \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
        "              train_acc, loss.item()) +\n",
        "          \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
        "              val_acc, val_loss.item()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.6 64-bit",
      "language": "python",
      "name": "python37664bit5a6158c51997408fa24508b198f2fb91"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}