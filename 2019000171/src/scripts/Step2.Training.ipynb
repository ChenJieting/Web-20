{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 构建训练集，测试集与验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "lines = open(\"uzi_data.txt\").readlines()\n",
    "for line in lines:\n",
    "    vector, label = line.strip().split(\"\\t\")\n",
    "    label = int(label)\n",
    "    if label == -1:\n",
    "        label = 0\n",
    "    vector = vector.split(\",\")\n",
    "    vector = [int(v) for v in vector]\n",
    "    data_x.append(vector)\n",
    "    data_y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = int(len(data_x)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(3684, 20) \n",
      "Train_Y set: \t(3684,) \n",
      "Test set: \t\t(1231, 20)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.array(data_x[:frac*3])\n",
    "train_y = np.array(data_y[:frac*3])\n",
    "valid_x = np.array(data_x[frac*3:frac*4])\n",
    "valid_y = np.array(data_y[frac*3:frac*4])\n",
    "test_x = np.array(data_x[frac*4:])\n",
    "test_y = np.array(data_y[frac*4:])\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nTrain_Y set: \\t{}\".format(train_y.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, drop_prob, bidirectional=False, attention=False):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.GPU = False\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob, bidirectional=bidirectional)\n",
    "        self.attention_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if bidirectional and not attention:\n",
    "            h_dim = hidden_dim * 2\n",
    "        else:\n",
    "            h_dim = hidden_dim\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(h_dim, 1),\n",
    "            nn.Dropout(drop_prob),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def attention_forward(self, x, hx):\n",
    "        if self.bidirectional:\n",
    "            x_tmp = torch.chunk(x, 2, -1)\n",
    "            h = x_tmp[0] + x_tmp[1]\n",
    "        else:\n",
    "            h = x\n",
    "        hx = torch.sum(hx, dim=1)\n",
    "        hx = hx.unsqueeze(1)\n",
    "        attent_w = self.attention_layers(hx)\n",
    "        m = torch.tanh(h)\n",
    "        attent_context = torch.bmm(attent_w, m.transpose(1, 2))\n",
    "        softmax_w = torch.softmax(attent_context, dim=-1)\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        context = context.squeeze(1)\n",
    "        return context\n",
    "        \n",
    "    def forward(self, x, hx):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        x = self.embedding(x)\n",
    "        x, hx = self.lstm(x, hx)\n",
    "        if self.attention == False:\n",
    "            if self.bidirectional:\n",
    "                x = x.contiguous().view(-1, self.hidden_dim*2)\n",
    "            else:\n",
    "                x = x.contiguous().view(-1, self.hidden_dim)\n",
    "            y = self.fc(x)\n",
    "            y = y.view(batch_size, -1)\n",
    "            y = y[:,-1]\n",
    "        else:\n",
    "           # x = x.permute(1, 0, 2)\n",
    "            hx = hx[0].permute(1, 0, 2)\n",
    "            attent_out = self.attention_forward(x, hx)\n",
    "            y = self.fc(attent_out)\n",
    "        return y\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        number = 2 if self.bidirectional else 1\n",
    "        if self.GPU:\n",
    "            hx = (weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hx = (weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers*number, batch_size, self.hidden_dim).zero_())\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"embedding_dim\": [64, 128, 256],\n",
    "    \"hidden_dim\": [100, 200, 300],\n",
    "    \"n_layers\": [1],\n",
    "    \"drop_prob\": [0.1, 0.2, 0.3],\n",
    "    \"lr\":[0.001, 0.002, 0.0005]\n",
    "}\n",
    "\n",
    "def param_search():\n",
    "    params = {}\n",
    "    for param in grid.keys():\n",
    "        params[param] = random.choice(grid[param])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_dim': 128, 'hidden_dim': 100, 'n_layers': 1, 'drop_prob': 0.1, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "params = param_search()\n",
    "epochs = 10\n",
    "lr = params[\"lr\"]\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(16296, 128)\n",
      "  (lstm): LSTM(128, 100, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (attention_layers): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "    (2): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SentimentLSTM(16296, params[\"embedding_dim\"], params[\"hidden_dim\"], params[\"n_layers\"], params[\"drop_prob\"],bidirectional=True,attention=True)\n",
    "print(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10... Step: 0... Loss: 0.685204...\n",
      "Epoch: 0/10... Step: 10... Loss: 0.584851...\n",
      "Epoch: 0/10... Step: 20... Loss: 0.627124...\n",
      "Epoch: 0/10... Step: 30... Loss: 0.541212...\n",
      "Epoch: 0/10... Step: 40... Loss: 0.556536...\n",
      "Epoch: 0/10... Step: 50... Loss: 0.513268...\n",
      "Epoch: 1/10... Step: 0... Loss: 0.576665...\n",
      "Epoch: 1/10... Step: 10... Loss: 0.469355...\n",
      "Epoch: 1/10... Step: 20... Loss: 0.585975...\n",
      "Epoch: 1/10... Step: 30... Loss: 0.443022...\n",
      "Epoch: 1/10... Step: 40... Loss: 0.499566...\n",
      "Epoch: 1/10... Step: 50... Loss: 0.557027...\n",
      "Epoch: 2/10... Step: 0... Loss: 0.439101...\n",
      "Epoch: 2/10... Step: 10... Loss: 0.423701...\n",
      "Epoch: 2/10... Step: 20... Loss: 0.414940...\n",
      "Epoch: 2/10... Step: 30... Loss: 0.494703...\n",
      "Epoch: 2/10... Step: 40... Loss: 0.373337...\n",
      "Epoch: 2/10... Step: 50... Loss: 0.353056...\n",
      "Epoch: 3/10... Step: 0... Loss: 0.447752...\n",
      "Epoch: 3/10... Step: 10... Loss: 0.415809...\n",
      "Epoch: 3/10... Step: 20... Loss: 0.419187...\n",
      "Epoch: 3/10... Step: 30... Loss: 0.258544...\n",
      "Epoch: 3/10... Step: 40... Loss: 0.243128...\n",
      "Epoch: 3/10... Step: 50... Loss: 0.401574...\n",
      "Epoch: 4/10... Step: 0... Loss: 0.195271...\n",
      "Epoch: 4/10... Step: 10... Loss: 0.226660...\n",
      "Epoch: 4/10... Step: 20... Loss: 0.379012...\n",
      "Epoch: 4/10... Step: 30... Loss: 0.355120...\n",
      "Epoch: 4/10... Step: 40... Loss: 0.305103...\n",
      "Epoch: 4/10... Step: 50... Loss: 0.269220...\n",
      "Epoch: 5/10... Step: 0... Loss: 0.186190...\n",
      "Epoch: 5/10... Step: 10... Loss: 0.188517...\n",
      "Epoch: 5/10... Step: 20... Loss: 0.265980...\n",
      "Epoch: 5/10... Step: 30... Loss: 0.179529...\n",
      "Epoch: 5/10... Step: 40... Loss: 0.144942...\n",
      "Epoch: 5/10... Step: 50... Loss: 0.269254...\n",
      "Epoch: 6/10... Step: 0... Loss: 0.196888...\n",
      "Epoch: 6/10... Step: 10... Loss: 0.196876...\n",
      "Epoch: 6/10... Step: 20... Loss: 0.322568...\n",
      "Epoch: 6/10... Step: 30... Loss: 0.183076...\n",
      "Epoch: 6/10... Step: 40... Loss: 0.148113...\n",
      "Epoch: 6/10... Step: 50... Loss: 0.191493...\n",
      "Epoch: 7/10... Step: 0... Loss: 0.086154...\n",
      "Epoch: 7/10... Step: 10... Loss: 0.134777...\n",
      "Epoch: 7/10... Step: 20... Loss: 0.090763...\n",
      "Epoch: 7/10... Step: 30... Loss: 0.094034...\n",
      "Epoch: 7/10... Step: 40... Loss: 0.061729...\n",
      "Epoch: 7/10... Step: 50... Loss: 0.121776...\n",
      "Epoch: 8/10... Step: 0... Loss: 0.081138...\n",
      "Epoch: 8/10... Step: 10... Loss: 0.099109...\n",
      "Epoch: 8/10... Step: 20... Loss: 0.114202...\n",
      "Epoch: 8/10... Step: 30... Loss: 0.068030...\n",
      "Epoch: 8/10... Step: 40... Loss: 0.143154...\n",
      "Epoch: 8/10... Step: 50... Loss: 0.135364...\n",
      "Epoch: 9/10... Step: 0... Loss: 0.098193...\n",
      "Epoch: 9/10... Step: 10... Loss: 0.054422...\n",
      "Epoch: 9/10... Step: 20... Loss: 0.092488...\n",
      "Epoch: 9/10... Step: 30... Loss: 0.092053...\n",
      "Epoch: 9/10... Step: 40... Loss: 0.048894...\n",
      "Epoch: 9/10... Step: 50... Loss: 0.098437...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    model.GPU = True\n",
    "\n",
    "model.train()\n",
    "\n",
    "t1 = time.time()\n",
    "for epoch in range(epochs):\n",
    "    it = 0\n",
    "    hx = model.init_hidden(batch_size)\n",
    "    for x, y in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        hx = tuple([h.data for h in hx])\n",
    "        y_ = model(x, hx)\n",
    "        loss = F.binary_cross_entropy(y_.squeeze(), y.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if it % 10 == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(epoch, epochs),\n",
    "                  \"Step: {}...\".format(it),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()))\n",
    "        it += 1\n",
    "t2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.775\n",
      "Accuracy: 0.764\n",
      "Time consuming: 34.440152406692505s\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "test_loss = []\n",
    "\n",
    "hx = model.init_hidden(batch_size)\n",
    " \n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for x, y in test_loader:\n",
    "    hx = tuple([h.data for h in hx])\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    y_ = model(x, hx)\n",
    "    \n",
    "    loss = F.binary_cross_entropy(y_.squeeze(), y.float())\n",
    "    test_loss.append(loss.item())\n",
    "    \n",
    "    pred = torch.round(y_.squeeze())\n",
    "    correct_tensor = pred.eq(y.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "                             \n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_loss)))\n",
    " \n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Accuracy: {:.3f}\".format(test_acc))\n",
    "print(\"Time consuming: {}s\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
