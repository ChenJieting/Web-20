# 使用BERT模型进行非典型的关系抽取任务
---
## 项目目标
本项目是来自于导师的一个项目，目标是对专业领域中句子级文本进行分类，且句子中存在一个实体，因此类似于关系抽取任务或者称为实体属性分类。由于该数据集还未标注完成，所以使用了一个散文的关系抽取数据集代替。

## 数据集介绍
本项目使用的是github上的一个[中文散文关系抽取数据集](https://github.com/lancopku/Chinese-Literature-NER-RE-Dataset)，自行处理后获得了2459条数据。由于数据集较小且主要是探索单实体的关系抽取的可行性，因此将数据集按9:1的比例，划分出2202条数据作为训练集，257条数据作为测试集。

该数据集结构如下，其中head表示第一个实体，tail表示第二个实体。在本项目中，tail会被忽略。

|index|head|tail|relation_type|sentence|
|-----|----|----|-------------|--------|

下图为数据集实例。
![数据集图片](https://s1.ax1x.com/2020/06/12/tX8gHK.png)
可以发现数据集中同一个句子存在不同实体对的关系，在tail被忽略的情况下，会出现相同输出对应不同输入的情况，给模型引入噪音。这个问题目前还未解决。

## 模型介绍
本项目使用的是google在2018年发表的一篇论文[《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》](https://arxiv.org/abs/1810.04805)中提出的BERT模型。BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder。BERT已经提出就在11项NLP任务上刷新了最好成绩，目前已经成为了NLP领域里程碑式的模型。
BERT的模型结构建立在[《attention is all you need》](https://arxiv.org/abs/1706.03762)提出的多层transformer基础之上，使用了transformer的encoder结构。
![transformer结构](https://s1.ax1x.com/2020/06/14/txHKts.png)
BERT分为pre-training阶段与fine-tuning阶段。在pre-training阶段，BERT模型在大型数据集上同时进行mask language model任务与next sentence prediction任务。通过pre-training阶段大批量语料的训练，模型获得了在公共领域理解文本内容的能力；在fine-tuning阶段，模型需要针对具体的下游nlp任务进行微调。
![BERT fine-tuning](https://s1.ax1x.com/2020/06/14/txLqED.jpg)
## 实验过程
本项目使用了两种输入方式，一种方法是直接在句子中使用特殊标注符将实体标注出来的句内标注方式；另一种是将句子与实体作为语句对进行输入的句外标注方式。
### 1.句中标注方式
这种方式参考了阿里团队在2019年发表的论文[《Enriching Pre-trained Language Model with Entity Information for Relation Classification》](https://arxiv.org/abs/1905.08284)。文章提出在句子内部使用"#"与"￥"标注出两个实体，并使用BERT直接处理标注后的句子的关系抽取方法。将bert输出的多层词向量对应实体位置的片段取平均，并与cls部分的句向量拼接，再进行softmax分类。本项目只需要标注一个实体，使用了"[E11]"与"[E12]"作为实体的前后标记符。在输出时同样是将实体对应位置的词向量取平均，并与句向量拼接后进行softmax分类。最终测试结果如下:

|pred\real|unknown|Create|Use|Near|Social|Located|Ownership|General-Special|Family|Part-Whole|
|-------|
|unknown|75|0|1|0|1|12|2|2|1|3|
|Create|0|2|0|0|0|0|0|0|0|0|
|Use|0|0|4|0|0|0|0|0|0|0|
|Near|0|0|0|8|0|1|0|0|0|0|
|Social|0|1|0|0|8|0|0|0|0|0|
|Located|0|0|0|0|0|69|0|0|0|0|
|Ownership|0|0|1|0|0|0|4|0|0|0|
|General-Special|0|0|0|0|0|0|0|6|1|0|
|Family|2|0|0|0|0|0|0|0|11|0|
|Part-Whole|1|0|0|0|0|0|1|1|0|39|

各类别的准确率、召回率与F1值为：
|eval\type|unknown|Create|Use|Near|Social|Located|Ownership|General-Special|Family|Part-Whole|
|-------|
|total|78|3|6|8|9|82|7|9|13|42|
|TP|75|2|4|8|8|69|4|6|11|39|
|FP|18|0|0|0|0|0|0|0|0|2|
|FN|3|1|2|0|1|13|3|3|2|3|
|precision|0.806|1.000|1.000|1.000|1.000|1.000|1.000|1.000|1.000|0.951|
|recall|0.962|0.667|0.667|1.000|0.889|0.841|0.571|0.667|0.846|0.929|
|F1|0.877|0.800|0.800|1.000|0.941|0.914|0.727|0.800|0.917|0.940|

**最终计算获得的准确率为：0.976，召回率为：0.803，F1值为：0.872。**
### 2.句外标注方式
第二种方式是将句子与实体组成实体对，同时输入BERT模型，并使用cls部分的句向量直接进行softmax分类。最终分类效果如下：
|pred\real|unknown|Create|Use|Near|Social|Located|Ownership|General-Special|Family|Part-Whole|
|-------|
|unknown|61|1|1|0|2|6|2|2|8|13|
|Create|0|1|0|0|0|0|0|0|0|0|
|Use|0|0|1|0|1|0|0|0|0|0|
|Near|1|0|0|6|0|1|0|0|0|0|
|Social|2|1|0|0|4|0|0|0|0|0|
|Located|5|0|1|2|0|69|1|0|0|3|
|Ownership|2|0|1|0|0|0|1|0|0|0|
|General-Special|0|0|0|0|1|0|0|5|0|0|
|Family|1|0|0|0|0|0|1|0|5|1|
|Part-Whole|6|0|2|0|1|6|2|2|0|25|

各类别的准确率、召回率与F1值为：
|eval\type|unknown|Create|Use|Near|Social|Located|Ownership|General-Special|Family|Part-Whole|
|-------|
|total|78|3|6|8|9|82|7|9|13|42|
|TP|61|1|1|6|4|69|1|5|5|25|
|FP|18|0|0|0|0|0|0|0|0|2|
|FN|17|2|5|2|5|13|6|4|8|17|
|precision|0.772|1|1|1|1|1|1|1|1|0.925|
|recall|0.782|0.333|0.167|0.75|0.444|0.841|0.142|0.556|0.384|0.595|
|F1|0.777|0.5|0.285|0.857|0.615|0.913|0.25|0.714|0.556|0.724|

**最终计算获得的precision为：0.970，recall为：0.500，F1值为：0.620。**

## 评价与实验总结
句内标注的效果明显好于句外标注，也证实了模型可以识别出句子中特殊标识符的含义。而句外标注方式表现不佳的原因，我认为是BERT模型对字词级文本的理解较差。如果句子中只有一个token，就相当于退化成了word2vec的静态词向量。其次，句子加实体构成语句对的形式与pre-training阶段的NSP训练也有比较大的差异，导致模型更难理解句子与实体的关系。
同时可以注意到前后两个模型使用同一个数据集，但易混淆的类型发生了变化，说明除了文本本身的差异，模型结构也会影响模型对文本内容的理解。
本项目可以改进的地方比较多，在数据集的处理上，如果提前对重复句进行筛选可以减少数据集的噪音；使用的特殊标识符"[E11]"在BERT中会被分词为"["、"E11"、"]"三个单独的token，如果将标识符精简为一个token可以降低模型识别的难度，提高句子质量；同时，本项目数据集较小，虽然将epoch设置为10，但模型明显仍有优化空间，但再增加epoch就存在过拟合的风险。

### 备注
本项目使用的bert预训练模型是[哈工大讯飞实验室联合发布的全词mask模型](https://github.com/ymcui/Chinese-BERT-wwm)，下载地址为http://pan.iflytek.com/#/link/5DBDD89414E5B565D3322D6B7937DF47，提取密码为hteX，下载后解压到src目录下的chinese-wwm-bert文件夹。